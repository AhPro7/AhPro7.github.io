<!DOCTYPE html>
<html lang="en">
  <head>

            <!-- Google tag (gtag.js) -->
            <script async src="https://www.googletagmanager.com/gtag/js?id=G-11N94LT6KX"></script>
            <script>
              window.dataLayer = window.dataLayer || [];
              function gtag(){dataLayer.push(arguments);}
              gtag('js', new Date());
    
              gtag('config', 'G-11N94LT6KX');
            </script>
    

    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>Ahmed Haytham</title>
    <link rel="icon" href="../assets/img/ahmed haytham-07.png" type="image/png" />

    <!-- meta -->
    <meta name="title" content="Text Generation" />
    <meta name="description" content="This article is about text generation theory using GPT-2 model. and how to use it in python." />
    <meta name="tags" content="text generation, gpt-2, python, ai, artificial intelligence, machine learning, deep learning, nlp" />
    <meta name="date" content="2023-7-30" />
    <meta name="img" content="/articles/images/text_genration.jpg" />

    <meta name="author" content="Ahmed Haytham" />
    <meta property="og:image" content="/articles/images/text_genration.jpg" />
    <meta property="og:title" content="Text Generation" />
    <meta property="og:description" content="Learn about text generation theory using GPT-2 model and how to use it in Python." />
    <meta property="og:type" content="article" />
    <meta property="og:site_name" content="Ahmed Haytham" />
    <meta property="og:locale" content="en_US" />
    <meta property="article:section" content="Technology" />
    <meta property="article:tag" content="text generation" />
    <meta property="article:tag" content="GPT-2" />
    <meta property="article:tag" content="Python" />

    <!-- fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Almarai:wght@300;400;700&display=swap"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/styles/default.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/highlight.min.js"></script>

    <link
      rel="stylesheet"
      type="text/css"
      href="https://use.fontawesome.com/releases/v5.3.1/css/all.css"
    />

    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <!-- styles -->
    <link rel="stylesheet" href="../assets/css/global.css" />
    <link rel="stylesheet" href="../assets/css/home/home.css" />
    <link rel="stylesheet" href="../assets/css/home/header.css" />
    <link rel="stylesheet" href="../assets/css/home/recent-articles.css" />
    <link rel="stylesheet" href="../assets/css/home/reviews.css" />
    <link rel="stylesheet" href="../assets/css/home/team.css" />
    <link rel="stylesheet" href="../assets/css/home/stats.css" />
    <link rel="stylesheet" href="../assets/css/home/news.css" />
    <link rel="stylesheet" href="../assets/css/home/feed.css" />
    <link rel="stylesheet" href="../assets/css/article/article.css" />

    <!-- javascript -->
    <script
      src="https://platform.linkedin.com/badges/js/profile.js"
      async
      defer
      type="text/javascript"
    ></script>
    <!-- <script src="../assets/js/stars.js" defer></script> -->
    <script src="../assets/js/header.js" defer></script>

  </head>
  <body>
    <div id="planet-container"></div>


    <!-- Navbar start -->
    <nav>
      <div class="inner-nav" dir="ltr">
        <div class="logo">
          <a href="../">
            <img src="../assets/img/ahmed haytham-07.png" alt="logo" />
          </a>
          <a href="../">
            <span><h3>Ahmed Haytham</h3></span>
          </a>
        </div>
        <div class="hambuger-button" onclick="toggleHamburger();">
          <div class="line line-1"></div>
          <div class="line line-2"></div>
          <div class="line line-3"></div>
        </div>
        <ul class="menu-closed">
          <a href="../"><li>Blog</li></a>

            <a onclick="hideandShowStarsandPlanets()" style="color: #cbd2dd; text-shadow: 0 0 2px #528eef, 0 0 4px #528eef, 0 0 6px #528eef, 0 0 8px #528eef; cursor: pointer;"><li>Animation</li></a>
        </ul>
      </div>
    </nav>
    <!-- Navbar End -->
    <main>
        <div class="article-title">
          <div class="title-container">
            <h1 class="typed">Text Generation</h1>
            <p style="font-size: 19px; margin-left: 10px; word-wrap: break-word;">Published on July 30, 2023</p>
          </div>
        </div>
        <img
          src="/articles/images/text_genration.jpg"
          alt=""
          class="article-banner"
        />
        <div class="article-body">

            <!-- article -->

            <h2>Introduction</h2>
            <p>
                Hello fellow adventurers, today we will be talking about text generation using GPT-2 model.<br>
                One of the most uncanny features of transformer-based language models is their ability to generate text that is almost indistinguishable from text written by humans. A famous example is OpenAI’s GPT-2, which when given this prompt
            </p>
            <blockquote>
                " In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously 
                unexplored valley, in the Andes Mountains. Even more surprising to the
                researchers was the fact that the unicorns spoke perfect English."
            </blockquote>
            <p>
                was able to generate a compelling news article about <b>talking unicorns:</b>
            </p>
            <!-- add line break -->
            <hr>
            <h3>What makes this example so remarkable?</h3>
            <p>
                is that it was generated without any explicit supervision! By simply learning to <b>predict the next word</b> in the <b>text of millions of web pages</b>
            </p>
            <hr/>
            <h2>Content :</h2>
                <ul class="content-ul">
                  <li style="margin-bottom: 8px;"><b>Seq 2 Seq Overview <mark>(you can skip this)</mark></b></li>
                  <li style="margin-bottom: 8px;"><b>Challenge with Generating Coherent Text</b></li>
                  <li style="margin-bottom: 8px;"><b>Decoding Methods</b></li>
                  <li style="margin-bottom: 8px;"><b>Measuring the Quality of Generated Text</b></li>
                </ul>
                <hr/>
            <h2>Seq 2 Seq Overview</h2>

            <p>
              Seq2Seq models are a class of models that are used to convert a sequence of one type into a sequence of another type. 
              They are used in a variety of tasks such as <b>machine translation</b>, <b>text summarization</b>, <b>question answering</b>, and <b>text generation</b>. <br/>
              <mark>
                let's say we have a sequence of words in English and we want to translate it to a sequence of words in French. this is a Seq2Seq problem. called <b>Machine Translation</b> which will be the example for this part.
              </mark>
              <br/>

              The basic idea behind Seq2Seq models is that we have two recurrent neural networks (RNNs) called the <b>encoder</b> and the <b>decoder</b> that work together to transform one sequence to another.<br/>
             <mark>  one model explain the input for the other model to help it generate the output</mark> 
            </p>

            <video src="https://jalammar.github.io/images/seq2seq_4.mp4" controls class="img_in_middle_article"></video>
            
            <p>
              <h3> Encoder </h3> 
              The encoder takes the input sequence and encodes it into a single vector called the <b>context vector</b>.<br/>
              <mark><b>ht=f(xt,ht−1)</b> &&    
              <b>c=q(h1,…,hT)</b></mark><br/>
              The encoder is a recurrent neural network (RNN), such as a GRU or LSTM. At each time step, the encoder produces a hidden state ht. The context vector c is then generated from the sequence of hidden states ht, using a nonlinear function q. The context vector c summarizes the entire input sequence and is used by the decoder to generate the output sequence.
              

            </p>

            <p>
              <h3> Decoder </h3>
              The decoder takes the <b>context vector</b> and generates the output sequence.<br/>
              The decoder is trained to predict the next word yt given the context vector c and all the previously predicted words {y1, ..., yt-1}. It defines a probability over the translation y by decomposing the joint probability:
              <mark>P(y)=∏Tt=1P(yt|y1,…,yt−1,c)</mark> this equation means that the probability of the next word yt is dependent on the previous words {y1, ..., yt-1} and the context vector c. In other words, the probability of a translation sequence is calculated by computing the conditional probability of each word given the previous words.
            </p> 
            <video src="https://jalammar.github.io/images/seq2seq_6.mp4" controls class="img_in_middle_article"></video>

            <p>
              <h3> Attention </h3>
              The attention mechanism is used to overcome the limitation of the encoder-decoder architecture. The encoder-decoder architecture encodes the entire input sequence into a single vector, which is then decoded into the output sequence. This architecture works well for <b>short sequences</b> but is <b>unable to generate good results for long sequences</b>. The attention mechanism overcomes this limitation by allowing the decoder to look at different parts of the input sequence at each step of the output generation. This is done by creating connections between the decoder and the encoder. The attention mechanism is a vector of weights that are multiplied with the encoder hidden states to get the context vector. The context vector is then concatenated with the decoder hidden state and fed into the decoder to generate the output.
              <mark>
                there is a lot of types of attention mechanism but we will not talk about them here.
              </mark>
            <video src="https://jalammar.github.io/images/attention_tensor_dance.mp4" controls class="img_in_middle_article"></video>
            <hr>
            <h2>The Challenge with Generating Coherent Text</h2>
            <p>
              For task-specific heads like sequence or token classification, generating predictions is fairly straightforward; the model produces some <b>logits</b> and we either take the maximum value to get the predicted class, or apply a <b>softmax</b> function to obtain the predicted probabilities per class <br/>
              But converting the model’s probabilistic output to text requires a <b>decoding method</b>, which introduces a few challenges that are unique to text generation
              <ul>
                <li>
                   The decoding is done iteratively and thus involves significantly more compute than simply passing inputs once through the forward pass of a model.
                </li>
                <li>
                   The quality and diversity of the generated text depend on the choice of decoding method and associated hyper-parameters.              </ul>
            </p>
            </p>
            <hr>
            
            <h2>Decoding Methods</h2>
            <h3> intro </h3>
            <p> 
              we will take GPT-2 as an example how it decode the output.
              <br/>
              <mark> If you skipped the previous part, please read the decoder section.</mark>
              <h3> more </h3>
              <p><strong>GPT-2</strong> is a language model that is trained to predict the next word in a sequence. It does this by estimating the probability of each possible next word, given the previous words in the sequence. This probability is called <strong>P(y|x)</strong>, where <strong>y</strong> is the next word and <strong>x</strong> is the sequence of previous words.</p>
              <mark>However, it is impractical to gather enough training data to estimate P(y|x) directly. This is because the number of possible sequences of words is astronomically large. Instead, GPT-2 uses the chain rule of probability to factorize P(y|x) as a product of conditional probabilities.</mark>
              <p>
                <strong>The chain rule</strong> of probability states that the probability of a sequence of events occurring is equal to the product of the probabilities of each event occurring, given that the previous events have already occurred. In the case of GPT-2, the sequence of events is the sequence of words.
                <br>
                <mark>P(w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>n</sub>) = P(w<sub>1</sub>) * P(w<sub>2</sub> | w<sub>1</sub>) * P(w<sub>3</sub> | w<sub>1</sub>, w<sub>2</sub>) * ... * P(w<sub>n</sub> | w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>n-1</sub>)</mark>
                <p>
                  For example, let's say we want to predict the next word in the sequence "I am a". The chain rule of probability tells us that the probability of this sequence occurring is equal to the probability of the word "a" occurring, given that the words "I" and "am" have already occurred, multiplied by the probability of the word "I" occurring, given that no words have previously occurred.
                </p>

                <p>
                  In other words, the probability of the sequence "I am a" occurring is equal to P("a" | "I am") * P("I" | "").
                </p>

                <p>
                  The chain rule of probability is a powerful tool that allows GPT-2 to estimate the probability of a sequence of words even when there is not enough training data to estimate P(y|x) directly.
                </p>

                <p>
                  Here are some examples of how the chain rule of probability can be used to predict the next word in a sequence:
                </p>

                <ul>
                  <li>
                    If the previous words are "I am", the probability of the next word being "a" is high. This is because the word "a" is a common word that often follows the word "I".
                  </li>
                  <li>
                    If the previous words are "I am a", the probability of the next word being "dog" is low. This is because the word "dog" is not a common word that follows the phrase "I am a".
                  </li>
                </ul>
                <img src="images/chain.png" class="img_in_middle_article">
                <!-- FILEPATH: /run/media/ahmed/Medo/Blog-Post/old/AhPro7.github.io/articles/text_gen.html -->
                <p>At the heart of this process lies a decoding method that determines which token is selected at each timestep. Since the language model head produces a <mark>logit zt</mark>, i per token in the vocabulary at each step, we can get the probability distribution over the next possible token <mark>wi</mark> by taking the <mark>softmax</mark>:</p>
                <mark>P(yₜ = wᵢ | y₍ₜ₋₁₎, x) = softmax(zₜ)</mark>
              </p>
            </p>
            <blockquote>The goal of most decoding methods is to search for the most likely overall sequence by picking a <mark>Y</mark> such that:</blockquote>
            <mark> Ŷ = argmax₍y₎ P(y|x)</mark><br/>
              Finding <strong>y-hat</strong> directly would involve evaluating every possible sequence with the language model. Since there does not exist an algorithm that can do this in a reasonable amount of time
            <hr>
            <h2>Decoding Methods</h2>
            <h3>1. Greedy Search Decoding</h3>
            <p>The simplest decoding method to get discrete tokens from a model’s continuous output is to
              greedily select the token with the highest probability at each timestep:</p>
            <mark>Ŷ = argmax₍y₎ P(y|x)</mark><br/>
            <p>
              Greedy search decoding is a simple decoding method that selects the token with the highest probability at each timestep. This method is computationally efficient, but it may not always produce the best results. The main drawback of greedy search decoding is that it tends to produce repetitive and generic text.
            </p>
            <p> let's see an example of greedy search decoding with GPT-2</p>

            <pre><code class="language-python">import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
device = "cuda" if torch.cuda.is_available() else "cpu"
model_name = "gpt2-xl"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name).to(device)</code></pre>
            
            <p> we get the model now we can use it to generate text</p> 
            <pre><code class="language-python">import pandas as pd

input_txt = "Transformers are the"
input_ids = tokenizer(input_txt, return_tensors="pt")["input_ids"].to(device)
iterations = []
n_steps = 8
choices_per_step = 5

with torch.no_grad():
  for _ in range(n_steps):
    iteration = dict()
    iteration["Input"] = tokenizer.decode(input_ids[0])
    output = model(input_ids=input_ids)
    # Select logits of the first batch and the last token and apply softmax
    next_token_logits = output.logits[0, -1, :]
    next_token_probs = torch.softmax(next_token_logits, dim=-1)
    sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)
    # Store tokens with highest probabilities
    for choice_idx in range(choices_per_step):
    token_id = sorted_ids[choice_idx]
    token_prob = next_token_probs[token_id].cpu().numpy()
    token_choice = (
    f"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)"
    )
    iteration[f"Choice {choice_idx+1}"] = token_choice
    # Append predicted next token to input
    input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)
    iterations.append(iteration)
pd.DataFrame(iterations)</code></pre>

<blockquote>Test it in Colab or Kaggle :) </blockquote>
            <p> now we will use the built-in function in transformers library to generate text</p>
            <pre><code class="language-python">input_ids = tokenizer(input_txt, return_tensors="pt")["input_ids"].to(device)
output = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)
print(tokenizer.decode(output[0]))</code></pre>
  <hr>
  <h3>2. Beam Search Decoding</h3>
  <p>Beam search decoding is a simple modification of greedy search decoding that keeps track of the <mark>k</mark> most likely sequences at each timestep, where <mark>k</mark> is a hyperparameter called the beam size. At each timestep, the beam search algorithm expands the <mark>k</mark> most likely sequences by adding all possible tokens to each sequence and keeping the <mark>k</mark> sequences with the highest probabilities. The beam search algorithm terminates when all <mark>k</mark> sequences end with the end-of-sequence token or when a maximum number of steps is reached.</p>

  <div>
    <ul>
      <li>Imagine you are trying to find the shortest path from one point to another on a map. You could start by following the path with the highest probability of being the shortest. But if you do that, you might miss out on other paths that are just as short or even shorter.</li>
      <li>A better way to find the shortest path is to consider all of the possible paths at each step. You can do this by keeping track of the top-b most probable paths, where b is the number of paths you want to consider.</li>
      <li>At each step, you can then choose the next path to follow by considering all of the possible extensions of the existing paths. You can then select the b most likely extensions.</li>
      <li>This process continues until you reach your destination.</li>
    </ul>
    <p>In the context of natural language processing, beam search is a technique used to generate text. It works by keeping track of the top-b most probable sequences of tokens at each step. At each step, the next token is chosen by considering all of the possible extensions of the existing sequences. The b most likely extensions are then selected.</p>
    <p>Beam search is a more complex technique than greedy search, but it can often produce better results. This is because beam search considers more possibilities at each step, which can help to prevent the model from getting stuck in local minima.</p>
    <p>Here is an example of how beam search works:</p>
    <ul>
      <li>Imagine you want to generate the sentence "The cat sat on the mat." The beam search algorithm would start by considering all of the possible sequences of tokens that start with the word "The." It would then choose the b most probable sequences.</li>
      <li>At the next step, the algorithm would consider all of the possible extensions of the b most probable sequences. It would then choose the b most probable extensions.</li>
      <li>This process would continue until the algorithm had generated the complete sentence "The cat sat on the mat."</li>
    </ul>
    <img class="img_in_middle_article" src="images/beam_search.png" alt="beam search">
    <p> let's see an example of beam search decoding with GPT-2</p> 
    <pre><code class="language-python">output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False)

logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))
print(tokenizer.decode(output_beam[0]))</code></pre>
<hr>
<h3>3. Sampling Decoding</h3>
<p>Sampling decoding is a technique used to generate text. It works by randomly selecting the next token from the probability distribution over the vocabulary. This technique is often used in conjunction with beam search decoding.</p>

  </div>
            </main>
    <script src="../assets/js/script.js"></script>
    <script>
      hljs.highlightAll();
    </script>
    <!-- <button onclick="hideandShowStarsandPlanets()">Delete Stars and Planets</button> -->
  </body>
</html>