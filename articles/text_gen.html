<!DOCTYPE html>
<html lang="en">
  <head>

            <!-- Google tag (gtag.js) -->
            <script async src="https://www.googletagmanager.com/gtag/js?id=G-11N94LT6KX"></script>
            <script>
              window.dataLayer = window.dataLayer || [];
              function gtag(){dataLayer.push(arguments);}
              gtag('js', new Date());
    
              gtag('config', 'G-11N94LT6KX');
            </script>
    

    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>Ahmed Haytham</title>
    <link rel="icon" href="../assets/img/ahmed haytham-07.png" type="image/png" />

    <!-- meta -->
    <meta name="title" content="Text Generation" />
    <meta name="description" content="This article is about text generation theory using GPT-2 model. and how to use it in python." />
    <meta name="tags" content="text generation, gpt-2, python, ai, artificial intelligence, machine learning, deep learning, nlp" />
    <meta name="date" content="2023-7-30" />
    <meta name="img" content="/articles/images/text_genration.jpg" />

    <meta name="author" content="Ahmed Haytham" />
    <meta property="og:image" content="/articles/images/text_genration.jpg" />
    <meta property="og:title" content="Text Generation" />
    <meta property="og:description" content="Learn about text generation theory using GPT-2 model and how to use it in Python." />
    <meta property="og:type" content="article" />
    <meta property="og:site_name" content="Ahmed Haytham" />
    <meta property="og:locale" content="en_US" />
    <meta property="article:section" content="Technology" />
    <meta property="article:tag" content="text generation" />
    <meta property="article:tag" content="GPT-2" />
    <meta property="article:tag" content="Python" />

    <!-- fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Almarai:wght@300;400;700&display=swap"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/styles/default.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/highlight.min.js"></script>

    <link
      rel="stylesheet"
      type="text/css"
      href="https://use.fontawesome.com/releases/v5.3.1/css/all.css"
    />

    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <!-- styles -->
    <link rel="stylesheet" href="../assets/css/global.css" />
    <link rel="stylesheet" href="../assets/css/home/home.css" />
    <link rel="stylesheet" href="../assets/css/home/header.css" />
    <link rel="stylesheet" href="../assets/css/home/recent-articles.css" />
    <link rel="stylesheet" href="../assets/css/home/reviews.css" />
    <link rel="stylesheet" href="../assets/css/home/team.css" />
    <link rel="stylesheet" href="../assets/css/home/stats.css" />
    <link rel="stylesheet" href="../assets/css/home/news.css" />
    <link rel="stylesheet" href="../assets/css/home/feed.css" />
    <link rel="stylesheet" href="../assets/css/article/article.css" />

    <!-- javascript -->
    <script
      src="https://platform.linkedin.com/badges/js/profile.js"
      async
      defer
      type="text/javascript"
    ></script>
    <script src="../assets/js/stars.js" defer></script>
    <script src="../assets/js/header.js" defer></script>

  </head>
  <body>
    <div id="planet-container"></div>


    <!-- Navbar start -->
    <nav>
      <div class="inner-nav" dir="ltr">
        <div class="logo">
          <a href="../">
            <img src="../assets/img/ahmed haytham-07.png" alt="logo" />
          </a>
          <a href="../">
            <span><h3>Ahmed Haytham</h3></span>
          </a>
        </div>
        <div class="hambuger-button" onclick="toggleHamburger();">
          <div class="line line-1"></div>
          <div class="line line-2"></div>
          <div class="line line-3"></div>
        </div>
        <ul class="menu-closed">
          <a href="../"><li>Blog</li></a>

            <a onclick="hideandShowStarsandPlanets()" style="color: #cbd2dd; text-shadow: 0 0 2px #528eef, 0 0 4px #528eef, 0 0 6px #528eef, 0 0 8px #528eef; cursor: pointer;"><li>Animation</li></a>
        </ul>
      </div>
    </nav>
    <!-- Navbar End -->
    <main>
        <div class="article-title">
          <div class="title-container">
            <h1 class="typed">Text Generation</h1>
            <p style="font-size: 19px; margin-left: 10px; word-wrap: break-word;">Published on July 30, 2023</p>
          </div>
        </div>
        <img
          src="/articles/images/text_genration.jpg"
          alt=""
          class="article-banner"
        />
        <div class="article-body">

            <!-- article -->

            <h2>Introduction</h2>
            <p>
                Hello fellow adventurers, today we will be talking about text generation using GPT-2 model.<br>
                One of the most uncanny features of transformer-based language models is their ability to generate text that is almost indistinguishable from text written by humans. A famous example is OpenAI’s GPT-2, which when given this prompt
            </p>
            <blockquote>
                " In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously 
                unexplored valley, in the Andes Mountains. Even more surprising to the
                researchers was the fact that the unicorns spoke perfect English."
            </blockquote>
            <p>
                was able to generate a compelling news article about <b>talking unicorns:</b>
            </p>
            <!-- add line break -->
            <hr>
            <h3>What makes this example so remarkable?</h3>
            <p>
                is that it was generated without any explicit supervision! By simply learning to <b>predict the next word</b> in the <b>text of millions of web pages</b>
            </p>
            <hr/>
            <h2>Content :</h2>
                <ul class="content-ul">
                  <li style="margin-bottom: 8px;"><b>Seq 2 Seq Overview <mark>(you can skip this)</mark></b></li>
                  <li style="margin-bottom: 8px;"><b>Challenge with Generating Coherent Text</b></li>
                  <li style="margin-bottom: 8px;"><b>Decoding Methods</b></li>
                  <li style="margin-bottom: 8px;"><b>Measuring the Quality of Generated Text</b></li>
                </ul>
                <hr/>
            <h2>Seq 2 Seq Overview</h2>

            <p>
              Seq2Seq models are a class of models that are used to convert a sequence of one type into a sequence of another type. 
              They are used in a variety of tasks such as <b>machine translation</b>, <b>text summarization</b>, <b>question answering</b>, and <b>text generation</b>. <br/>
              <mark>
                let's say we have a sequence of words in English and we want to translate it to a sequence of words in French. this is a Seq2Seq problem. called <b>Machine Translation</b> which will be the example for this part.
              </mark>
              <br/>

              The basic idea behind Seq2Seq models is that we have two recurrent neural networks (RNNs) called the <b>encoder</b> and the <b>decoder</b> that work together to transform one sequence to another.<br/>
             <mark>  one model explain the input for the other model to help it generate the output</mark> 
            </p>

            <video src="https://jalammar.github.io/images/seq2seq_4.mp4" controls class="img_in_middle_article"></video>
            
            <p>
              <h3> Encoder </h3> 
              The encoder takes the input sequence and encodes it into a single vector called the <b>context vector</b>.<br/>
              <mark><b>ht=f(xt,ht−1)</b> &&    
              <b>c=q(h1,…,hT)</b></mark><br/>
              The encoder is a recurrent neural network (RNN), such as a GRU or LSTM. At each time step, the encoder produces a hidden state ht. The context vector c is then generated from the sequence of hidden states ht, using a nonlinear function q. The context vector c summarizes the entire input sequence and is used by the decoder to generate the output sequence.
              

            </p>

            <p>
              <h3> Decoder </h3>
              The decoder takes the <b>context vector</b> and generates the output sequence.<br/>
              The decoder is trained to predict the next word yt given the context vector c and all the previously predicted words {y1, ..., yt-1}. It defines a probability over the translation y by decomposing the joint probability:
              <mark>P(y)=∏Tt=1P(yt|y1,…,yt−1,c)</mark> this equation means that the probability of the next word yt is dependent on the previous words {y1, ..., yt-1} and the context vector c. In other words, the probability of a translation sequence is calculated by computing the conditional probability of each word given the previous words.
            </p> 
            <video src="https://jalammar.github.io/images/seq2seq_6.mp4" controls class="img_in_middle_article"></video>

            <p>
              <h3> Attention </h3>
              The attention mechanism is used to overcome the limitation of the encoder-decoder architecture. The encoder-decoder architecture encodes the entire input sequence into a single vector, which is then decoded into the output sequence. This architecture works well for <b>short sequences</b> but is <b>unable to generate good results for long sequences</b>. The attention mechanism overcomes this limitation by allowing the decoder to look at different parts of the input sequence at each step of the output generation. This is done by creating connections between the decoder and the encoder. The attention mechanism is a vector of weights that are multiplied with the encoder hidden states to get the context vector. The context vector is then concatenated with the decoder hidden state and fed into the decoder to generate the output.
              <mark>
                there is a lot of types of attention mechanism but we will not talk about them here.
              </mark>
            <video src="https://jalammar.github.io/images/attention_tensor_dance.mp4" controls class="img_in_middle_article"></video>
            <hr>
            <h2>Challenge with Generating Coherent Text</h2>
            <p>
              For task-specific heads like sequence or token classification, generating predictions is fairly straightforward; the model produces some <b>logits</b> and we either take the maximum value to get the predicted class, or apply a <b>softmax</b> function to obtain the predicted probabilities per class <br/>
              But converting the model’s probabilistic output to text requires a <b>decoding method</b>, which introduces a few challenges that are unique to text generation
              <ul>
                <li>
                   The decoding is done iteratively and thus involves significantly more compute than simply passing inputs once through the forward pass of a model.
                </li>
                <li>
                   The quality and diversity of the generated text depend on the choice of decoding method and associated hyper-parameters.              </ul>
            </p>

        </div>
    </main>
    
    <script src="../assets/js/script.js"></script>
    <script>
      hljs.highlightAll();
    </script>
    <!-- <button onclick="hideandShowStarsandPlanets()">Delete Stars and Planets</button> -->
  </body>
</html>