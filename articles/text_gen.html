<!DOCTYPE html>
<html lang="en">
  <head>

            <!-- Google tag (gtag.js) -->
            <script async src="https://www.googletagmanager.com/gtag/js?id=G-11N94LT6KX"></script>
            <script>
              window.dataLayer = window.dataLayer || [];
              function gtag(){dataLayer.push(arguments);}
              gtag('js', new Date());
    
              gtag('config', 'G-11N94LT6KX');
            </script>
    

    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>Ahmed Haytham</title>
    <link rel="icon" href="../assets/img/ahmed haytham-07.png" type="image/png" />

    <!-- meta -->
    <meta name="title" content="Text Generation" />
    <meta name="description" content="This article is about text generation theory using GPT-2 model. and how to use it in python." />
    <meta name="tags" content="text generation, gpt-2, python, ai, artificial intelligence, machine learning, deep learning, nlp" />
    <meta name="date" content="2023-7-30" />
    <meta name="img" content="/articles/images/text_genration.jpg" />

    <meta name="author" content="Ahmed Haytham" />
    <meta property="og:image" content="/articles/images/text_genration.jpg" />
    <meta property="og:title" content="Text Generation" />
    <meta property="og:description" content="Learn about text generation theory using GPT-2 model and how to use it in Python." />
    <meta property="og:type" content="article" />
    <meta property="og:site_name" content="Ahmed Haytham" />
    <meta property="og:locale" content="en_US" />
    <meta property="article:section" content="Technology" />
    <meta property="article:tag" content="text generation" />
    <meta property="article:tag" content="GPT-2" />
    <meta property="article:tag" content="Python" />

    <!-- fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=Almarai:wght@300;400;700&display=swap"
      rel="stylesheet"
    />

    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/styles/default.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.2.0/highlight.min.js"></script>

    <link
      rel="stylesheet"
      type="text/css"
      href="https://use.fontawesome.com/releases/v5.3.1/css/all.css"
    />

    <meta name="viewport" content="width=device-width, initial-scale=1" />

    <!-- styles -->
    <link rel="stylesheet" href="../assets/css/global.css" />
    <link rel="stylesheet" href="../assets/css/home/home.css" />
    <link rel="stylesheet" href="../assets/css/home/header.css" />
    <link rel="stylesheet" href="../assets/css/home/recent-articles.css" />
    <link rel="stylesheet" href="../assets/css/home/reviews.css" />
    <link rel="stylesheet" href="../assets/css/home/team.css" />
    <link rel="stylesheet" href="../assets/css/home/stats.css" />
    <link rel="stylesheet" href="../assets/css/home/news.css" />
    <link rel="stylesheet" href="../assets/css/home/feed.css" />
    <link rel="stylesheet" href="../assets/css/article/article.css" />

    <!-- javascript -->
    <script
      src="https://platform.linkedin.com/badges/js/profile.js"
      async
      defer
      type="text/javascript"
    ></script>
    <!-- <script src="../assets/js/stars.js" defer></script> -->
    <script src="../assets/js/header.js" defer></script>

  </head>
  <body>
    <div id="planet-container"></div>


    <!-- Navbar start -->
    <nav>
      <div class="inner-nav" dir="ltr">
        <div class="logo">
          <a href="../">
            <img src="../assets/img/ahmed haytham-07.png" alt="logo" />
          </a>
          <a href="../">
            <span><h3>Ahmed Haytham</h3></span>
          </a>
        </div>
        <div class="hambuger-button" onclick="toggleHamburger();">
          <div class="line line-1"></div>
          <div class="line line-2"></div>
          <div class="line line-3"></div>
        </div>
        <ul class="menu-closed">
          <a href="../"><li>Blog</li></a>

            <a onclick="hideandShowStarsandPlanets()" style="color: #cbd2dd; text-shadow: 0 0 2px #528eef, 0 0 4px #528eef, 0 0 6px #528eef, 0 0 8px #528eef; cursor: pointer;"><li>Animation</li></a>
        </ul>
      </div>
    </nav>
    <!-- Navbar End -->
    <main>
        <div class="article-title">
          <div class="title-container">
            <h1 class="typed">Text Generation</h1>
            <p style="font-size: 19px; margin-left: 10px; word-wrap: break-word;">Published on July 30, 2023</p>
          </div>
        </div>
        <img
          src="/articles/images/text_genration.jpg"
          alt=""
          class="article-banner"
        />
        <div class="article-body">

            <!-- article -->

            <h2>Introduction</h2>
            <p>
                Hello fellow adventurers, today we will be talking about text generation using GPT-2 model.<br>
                One of the most uncanny features of transformer-based language models is their ability to generate text that is almost indistinguishable from text written by humans. A famous example is OpenAI’s GPT-2, which when given this prompt
            </p>
            <blockquote>
                " In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously 
                unexplored valley, in the Andes Mountains. Even more surprising to the
                researchers was the fact that the unicorns spoke perfect English."
            </blockquote>
            <p>
                was able to generate a compelling news article about <b>talking unicorns:</b>
            </p>
            <!-- add line break -->
            <hr>
            <h3>What makes this example so remarkable?</h3>
            <p>
                is that it was generated without any explicit supervision! By simply learning to <b>predict the next word</b> in the <b>text of millions of web pages</b>
            </p>
            <hr/>
            <h2>Content :</h2>
                <ul class="content-ul">
                  <li style="margin-bottom: 8px;"><b>Seq 2 Seq Overview <mark>(you can skip this)</mark></b></li>
                  <li style="margin-bottom: 8px;"><b>Challenge with Generating Coherent Text</b></li>
                  <li style="margin-bottom: 8px;"><b>Decoding Methods</b></li>
                  <li style="margin-bottom: 8px;"><b>Measuring the Quality of Generated Text</b></li>
                </ul>
                <hr/>
            <h2>Seq 2 Seq Overview</h2>

            <p>
              Seq2Seq models are a class of models that are used to convert a sequence of one type into a sequence of another type. 
              They are used in a variety of tasks such as <b>machine translation</b>, <b>text summarization</b>, <b>question answering</b>, and <b>text generation</b>. <br/>
              <mark>
                let's say we have a sequence of words in English and we want to translate it to a sequence of words in French. this is a Seq2Seq problem. called <b>Machine Translation</b> which will be the example for this part.
              </mark>
              <br/>

              The basic idea behind Seq2Seq models is that we have two recurrent neural networks (RNNs) called the <b>encoder</b> and the <b>decoder</b> that work together to transform one sequence to another.<br/>
             <mark>  one model explain the input for the other model to help it generate the output</mark> 
            </p>

            <video src="https://jalammar.github.io/images/seq2seq_4.mp4" controls class="img_in_middle_article"></video>
            
            <p>
              <h3> Encoder </h3> 
              The encoder takes the input sequence and encodes it into a single vector called the <b>context vector</b>.<br/>
              <mark><b>ht=f(xt,ht−1)</b> &&    
              <b>c=q(h1,…,hT)</b></mark><br/>
              The encoder is a recurrent neural network (RNN), such as a GRU or LSTM. At each time step, the encoder produces a hidden state ht. The context vector c is then generated from the sequence of hidden states ht, using a nonlinear function q. The context vector c summarizes the entire input sequence and is used by the decoder to generate the output sequence.
              

            </p>

            <p>
              <h3> Decoder </h3>
              The decoder takes the <b>context vector</b> and generates the output sequence.<br/>
              The decoder is trained to predict the next word yt given the context vector c and all the previously predicted words {y1, ..., yt-1}. It defines a probability over the translation y by decomposing the joint probability:
              <mark>P(y)=∏Tt=1P(yt|y1,…,yt−1,c)</mark> this equation means that the probability of the next word yt is dependent on the previous words {y1, ..., yt-1} and the context vector c. In other words, the probability of a translation sequence is calculated by computing the conditional probability of each word given the previous words.
            </p> 
            <video src="https://jalammar.github.io/images/seq2seq_6.mp4" controls class="img_in_middle_article"></video>

            <p>
              <h3> Attention </h3>
              The attention mechanism is used to overcome the limitation of the encoder-decoder architecture. The encoder-decoder architecture encodes the entire input sequence into a single vector, which is then decoded into the output sequence. This architecture works well for <b>short sequences</b> but is <b>unable to generate good results for long sequences</b>. The attention mechanism overcomes this limitation by allowing the decoder to look at different parts of the input sequence at each step of the output generation. This is done by creating connections between the decoder and the encoder. The attention mechanism is a vector of weights that are multiplied with the encoder hidden states to get the context vector. The context vector is then concatenated with the decoder hidden state and fed into the decoder to generate the output.
              <mark>
                there is a lot of types of attention mechanism but we will not talk about them here.
              </mark>
            <video src="https://jalammar.github.io/images/attention_tensor_dance.mp4" controls class="img_in_middle_article"></video>
            <hr>
            <h2>The Challenge with Generating Coherent Text</h2>
            <p>
              For task-specific heads like sequence or token classification, generating predictions is fairly straightforward; the model produces some <b>logits</b> and we either take the maximum value to get the predicted class, or apply a <b>softmax</b> function to obtain the predicted probabilities per class <br/>
              But converting the model’s probabilistic output to text requires a <b>decoding method</b>, which introduces a few challenges that are unique to text generation
              <ul>
                <li>
                   The decoding is done iteratively and thus involves significantly more compute than simply passing inputs once through the forward pass of a model.
                </li>
                <li>
                   The quality and diversity of the generated text depend on the choice of decoding method and associated hyper-parameters.              </ul>
            </p>
            </p>
            <hr>
            
            <h2>Decoding Methods</h2>
            <h3> intro </h3>
            <p> 
              we will take GPT-2 as an example how it decode the output.
              <br/>
              <mark> If you skipped the previous part, please read the decoder section.</mark>
              <h3> more </h3>
              <p><strong>GPT-2</strong> is a language model that is trained to predict the next word in a sequence. It does this by estimating the probability of each possible next word, given the previous words in the sequence. This probability is called <strong>P(y|x)</strong>, where <strong>y</strong> is the next word and <strong>x</strong> is the sequence of previous words.</p>
              <mark>However, it is impractical to gather enough training data to estimate P(y|x) directly. This is because the number of possible sequences of words is astronomically large. Instead, GPT-2 uses the chain rule of probability to factorize P(y|x) as a product of conditional probabilities.</mark>
              <p>
                <strong>The chain rule</strong> of probability states that the probability of a sequence of events occurring is equal to the product of the probabilities of each event occurring, given that the previous events have already occurred. In the case of GPT-2, the sequence of events is the sequence of words.
                <br>
                <mark>P(w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>n</sub>) = P(w<sub>1</sub>) * P(w<sub>2</sub> | w<sub>1</sub>) * P(w<sub>3</sub> | w<sub>1</sub>, w<sub>2</sub>) * ... * P(w<sub>n</sub> | w<sub>1</sub>, w<sub>2</sub>, ..., w<sub>n-1</sub>)</mark>
                <p>
                  For example, let's say we want to predict the next word in the sequence "I am a". The chain rule of probability tells us that the probability of this sequence occurring is equal to the probability of the word "a" occurring, given that the words "I" and "am" have already occurred, multiplied by the probability of the word "I" occurring, given that no words have previously occurred.
                </p>

                <p>
                  In other words, the probability of the sequence "I am a" occurring is equal to P("a" | "I am") * P("I" | "").
                </p>

                <p>
                  The chain rule of probability is a powerful tool that allows GPT-2 to estimate the probability of a sequence of words even when there is not enough training data to estimate P(y|x) directly.
                </p>

                <p>
                  Here are some examples of how the chain rule of probability can be used to predict the next word in a sequence:
                </p>

                <ul>
                  <li>
                    If the previous words are "I am", the probability of the next word being "a" is high. This is because the word "a" is a common word that often follows the word "I".
                  </li>
                  <li>
                    If the previous words are "I am a", the probability of the next word being "dog" is low. This is because the word "dog" is not a common word that follows the phrase "I am a".
                  </li>
                </ul>
                <img src="images/chain.png" class="img_in_middle_article">
                <!-- FILEPATH: /run/media/ahmed/Medo/Blog-Post/old/AhPro7.github.io/articles/text_gen.html -->
                <p>At the heart of this process lies a decoding method that determines which token is selected at each timestep. Since the language model head produces a <mark>logit zt</mark>, i per token in the vocabulary at each step, we can get the probability distribution over the next possible token <mark>wi</mark> by taking the <mark>softmax</mark>:</p>
                <mark>P(yₜ = wᵢ | y₍ₜ₋₁₎, x) = softmax(zₜ)</mark>
              </p>
            </p>
            <blockquote>The goal of most decoding methods is to search for the most likely overall sequence by picking a <mark>Y</mark> such that:</blockquote>
            <mark> Ŷ = argmax₍y₎ P(y|x)</mark><br/>
              Finding <strong>y-hat</strong> directly would involve evaluating every possible sequence with the language model. Since there does not exist an algorithm that can do this in a reasonable amount of time
            <hr>
            <h2>Decoding Methods</h2>
            <h3>1. Greedy Search Decoding</h3>
            <p>The simplest decoding method to get discrete tokens from a model’s continuous output is to
              greedily select the token with the highest probability at each timestep:</p>
            <mark>Ŷ = argmax₍y₎ P(y|x)</mark><br/>
            <p>
              Greedy search decoding is a simple decoding method that selects the token with the highest probability at each timestep. This method is computationally efficient, but it may not always produce the best results. The main drawback of greedy search decoding is that it tends to produce repetitive and generic text.
            </p>
            <p> let's see an example of greedy search decoding with GPT-2</p>

            <pre><code class="language-python">import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
device = "cuda" if torch.cuda.is_available() else "cpu"
model_name = "gpt2-xl"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name).to(device)</code></pre>
            
            <p> we get the model now we can use it to generate text</p> 
            <pre><code class="language-python">import pandas as pd

input_txt = "Transformers are the"
input_ids = tokenizer(input_txt, return_tensors="pt")["input_ids"].to(device)
iterations = []
n_steps = 8
choices_per_step = 5

with torch.no_grad():
  for _ in range(n_steps):
    iteration = dict()
    iteration["Input"] = tokenizer.decode(input_ids[0])
    output = model(input_ids=input_ids)
    # Select logits of the first batch and the last token and apply softmax
    next_token_logits = output.logits[0, -1, :]
    next_token_probs = torch.softmax(next_token_logits, dim=-1)
    sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)
    # Store tokens with highest probabilities
    for choice_idx in range(choices_per_step):
    token_id = sorted_ids[choice_idx]
    token_prob = next_token_probs[token_id].cpu().numpy()
    token_choice = (
    f"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)"
    )
    iteration[f"Choice {choice_idx+1}"] = token_choice
    # Append predicted next token to input
    input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)
    iterations.append(iteration)
pd.DataFrame(iterations)</code></pre>

<blockquote>Test it in Colab or Kaggle :) </blockquote>
            <p> now we will use the built-in function in transformers library to generate text</p>
            <pre><code class="language-python">input_ids = tokenizer(input_txt, return_tensors="pt")["input_ids"].to(device)
output = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)
print(tokenizer.decode(output[0]))</code></pre>
  <hr>
  <h3>2. Beam Search Decoding</h3>
  <p>Beam search decoding is a simple modification of greedy search decoding that keeps track of the <mark>k</mark> most likely sequences at each timestep, where <mark>k</mark> is a hyperparameter called the beam size. At each timestep, the beam search algorithm expands the <mark>k</mark> most likely sequences by adding all possible tokens to each sequence and keeping the <mark>k</mark> sequences with the highest probabilities. The beam search algorithm terminates when all <mark>k</mark> sequences end with the end-of-sequence token or when a maximum number of steps is reached.</p>

  <div>
    <ul>
      <li>Imagine you are trying to find the shortest path from one point to another on a map. You could start by following the path with the highest probability of being the shortest. But if you do that, you might miss out on other paths that are just as short or even shorter.</li>
      <li>A better way to find the shortest path is to consider all of the possible paths at each step. You can do this by keeping track of the top-b most probable paths, where b is the number of paths you want to consider.</li>
      <li>At each step, you can then choose the next path to follow by considering all of the possible extensions of the existing paths. You can then select the b most likely extensions.</li>
      <li>This process continues until you reach your destination.</li>
    </ul>
    <p>In the context of natural language processing, beam search is a technique used to generate text. It works by keeping track of the top-b most probable sequences of tokens at each step. At each step, the next token is chosen by considering all of the possible extensions of the existing sequences. The b most likely extensions are then selected.</p>
    <p>Beam search is a more complex technique than greedy search, but it can often produce better results. This is because beam search considers more possibilities at each step, which can help to prevent the model from getting stuck in local minima.</p>
    <p>Here is an example of how beam search works:</p>
    <ul>
      <li>Imagine you want to generate the sentence "The cat sat on the mat." The beam search algorithm would start by considering all of the possible sequences of tokens that start with the word "The." It would then choose the b most probable sequences.</li>
      <li>At the next step, the algorithm would consider all of the possible extensions of the b most probable sequences. It would then choose the b most probable extensions.</li>
      <li>This process would continue until the algorithm had generated the complete sentence "The cat sat on the mat."</li>
    </ul>
    <img class="img_in_middle_article" src="images/beam_search.png" alt="beam search">
    <p> let's see an example of beam search decoding with GPT-2</p> 
    <pre><code class="language-python">output_beam = model.generate(input_ids, max_length=max_length, num_beams=5, do_sample=False)

logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))
print(tokenizer.decode(output_beam[0]))</code></pre>
<hr>
<h3>3. Sampling Decoding</h3>
<p>Sampling decoding is a technique used to generate text. It works by randomly selecting the next token from the probability distribution over the vocabulary. This technique is often used in conjunction with beam search decoding.</p>

<p><mark>P (y<sub>t</sub> = w<sub>i</sub> | y<sub>&lt;t</sub>, X) = <strong>softmax(z<sub>t</sub>, i)</strong> = <sup>exp(z<sub>ti</sub>)</sup>&frasl;<sub>Σ<sub>j</sub> exp(z<sub>tj</sub>)</sub></mark></p>

<p>where |V| denotes the cardinality of the vocabulary. We can easily control the diversity of the output by adding a temperature parameter T that rescales the logits before taking the softmax:</p>
<p><mark>P(y<sub>t</sub> = w<sub>i</sub> | y<sub>&lt;t,x</sub>) = <sup>exp(z<sub>t,i/T</sub>)</sup>&frasl;<sub>Σ<sub>j=1</sub> exp(z<sub>tj/T</sub>)</sub></mark></p>
<p>The formula takes the logits from the language model and scales them by the temperature parameter. The scaled logits are then passed through a softmax function to produce a probability distribution over the tokens.</p>

<p>Temperature sampling addresses this problem by adding a <strong>temperature parameter</strong> to the output distribution. The temperature parameter controls how "smooth" the distribution is. When the temperature is low, the distribution is peaked around the most likely tokens, which makes the outputs more repetitive. When the temperature is high, the distribution is flatter, which makes the outputs more diverse.</p>
<img class="img_in_middle_article" src="images/Tem.png" alt="temperature sampling">

<p>In plain English, the formula says that the probability of a token being chosen is proportional to the exponent of the token's logit, divided by the sum of the exponents of all the tokens. The temperature parameter controls the exponents, so it controls the shape of the probability distribution.</p>

<p>Here is an example of how temperature sampling works. Let's say we have a language model that has been trained on a dataset of news articles. We want to use the language model to generate a new news article.</p>

<p>If we set the temperature parameter to a low value, the language model will be more likely to generate an article that is similar to the articles in the training dataset. This is because the low temperature will cause the probability distribution to be peaked around the most likely tokens, which are the tokens that appear frequently in the training dataset.</p>

<p>If we set the temperature parameter to a high value, the language model will be more likely to generate an article that is different from the articles in the training dataset. This is because the high temperature will cause the probability distribution to be flatter, which means that all the tokens will have a chance of being chosen.</p>

<p>The effect of temperature on token probabilities is shown in the image you sent me. The image shows the probability of a token being chosen as a function of the temperature parameter. As the temperature increases, the probability of all the tokens increases, but the probability of the most likely tokens decreases.</p>

<pre><code class="language-python">output_temp = model.generate(input_ids, max_length=max_length, do_sample=True,temperature=2.0, top_k=0)
print(tokenizer.decode(output_temp[0]))</code></pre>
<mark>change the temperature and see the difference</mark>
<hr>
<h3>4. Top-K and Nucleus Sampling</h3>
<p>Top-k and nucleus (top-p) sampling are two popular alternatives or extensions tousing temperature.</p>


<p>in both cases, the basic idea is to restrict the number of possible tokens we can sample from at each timestep</p>
<p>To see how this works, let’s first visualize <mark><strong>The cumulative probability</strong></mark> distribution of the model’s outputs at <strong>T = 1</strong> as seen in</p>
<img class="img_in_middle_article" src="images/Top_K.png" alt="cumulative probability">

<p><em>Let’s tease apart these plots, since they contain a lot of information.</em></p>
<p>The upper plot shows a histogram of the token probabilities. A <strong>histogram</strong> is a graph that shows the distribution of data. In this case, the data is the probabilities of the tokens in the vocabulary.</p>
<p>The histogram has two peaks. The first peak is around 10<sup>-8</sup>, which means that there are a lot of tokens with a probability of about 1 in 100 million. The second peak is around 10<sup>-4</sup>, which means that there are a few more tokens with a probability of about 1 in 10,000.</p>
<p>After the second peak, there is a sharp drop. This means that there are only a handful of tokens with a probability between 10<sup>-2</sup> and 10<sup>-1</sup>. The isolated bar at 10<sup>-1</sup> represents the token with the highest probability. The probability of picking this token is 1 in 10.</p>
<p>In other words, the most likely token has a probability of 1 in 10. There are a few other tokens that are also relatively likely, but the vast majority of tokens have a very low probability.</p>


<p><strong>The lower plot</strong> shows the cumulative probability of picking a token from the vocabulary. The cumulative probability is the probability of picking a token and all the tokens that come before it.</p>
<img class="img_in_middle_article" src="images/cul.png" alt="cumulative probability">

<p>The plot shows that there is a 96% chance of picking any of the 1,000 tokens with the highest probability. This means that if you randomly pick a token, there is a 96% chance that it will be one of the 1,000 most likely tokens.</p>

  <p>The probability rises quickly above 90%, but it takes several thousand tokens to reach close to 100%. This means that there is a 1 in 100 chance of not picking any of the tokens that are not even in the top 2,000.</p>

  <p>These numbers might seem small at first, but they become important when you consider that we sample once per token when generating text. So even if there is only a 1 in 100 or 1,000 chance of picking an unlikely token, if we sample hundreds of times, there is a significant chance of picking an unlikely token at some point.</p>

  <p>This can badly influence the quality of the generated text. For this reason, we generally want to avoid these very unlikely tokens. This is where top-k and top-p sampling come into play.</p>
<pre><code class="language-python">output_topk = model.generate(input_ids, max_length=max_length, do_sample=True,top_k=50)
print(tokenizer.decode(output_topk[0]))

output_topp = model.generate(input_ids, max_length=max_length, do_sample=True,top_p=0.90)
print(tokenizer.decode(output_topp[0]))</code></pre>

<mark>Top-k sampling is a technique that avoids the low-probability choices by only sampling from the k tokens with the highest probability. For example, if we set k to 100, then we will only sample from the 100 most likely tokens.</mark>
<br><br>

<mark>Top-p sampling is a similar technique that avoids the low-probability choices by only sampling from the tokens whose cumulative probability is greater than or equal to p. For example, if we set p to 0.95, then we will only sample from the tokens whose cumulative probability is greater than or equal to 95%.</mark>
<blockquote>We can also apply beam search when we use sampling. Instead of
  selecting the next batch of candidate tokens greedily, we can sample
  them and build up the beams in the same way</blockquote>
<hr>
<h2>Which Decoding Method Is Best?</h2>
<p>Unfortunately, There is no universally "best" decoding method for language models. The best approach will depend on the nature of the task you are generating text for.</p>
<ul>
  <li>If you want your model to perform a precise task, such as arithmetic or providing an answer to a specific question, you should lower the temperature or use deterministic methods like greedy search in combination with beam search. This will guarantee that the model always generates the most likely answer.</li>
  <li>If you want the model to generate longer texts and even be a bit creative, you should switch to sampling methods and increase the temperature. This will allow the model to explore more possibilities and generate more diverse text. You can also use a mix of top-k and nucleus sampling to further control the diversity of the generated text.</li>
</ul>
<hr>
<h2>Measuring the Quality of Generated Text</h2>
<p>
  this section will be in the next article. as this article is too long.

</p>
<hr>
<h2 style="color: rgb(17, 124, 255);">References</h2>
<ul>
  <li><a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)</a></li>
  <li><a href="https://www.oreilly.com/library/view/natural-language-processing/9781098136789/">Natural Language Processing with Transformers</a></li>


</main>
    <script src="../assets/js/script.js"></script>
    <script>
      hljs.highlightAll();
    </script>
    <!-- <button onclick="hideandShowStarsandPlanets()">Delete Stars and Planets</button> -->
  </body>
</html>